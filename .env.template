# Performance configuration
# Number of worker threads for processing scan requests (default: 4)
# Adjust based on your server's CPU cores and expected load
THREAD_POOL_WORKERS=4

# Hugging Face API configuration (REQUIRED)
# Get your token from: https://huggingface.co/settings/tokens
# Make sure you have access to: https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M
HF_TOKEN=your_huggingface_token_here

# Together API configuration (Optional)
# Required only if using PII_DETECTION scanner
# Get your API key from: https://www.together.ai/
TOGETHER_API_KEY=your_together_api_key_here

# OpenAI API configuration (required if using MODERATION scanner)
# Get your key from: https://platform.openai.com/api-keys
# Your account must be funded to be able to use the moderation endpoint
OPENAI_API_KEY=your_openai_api_key_here

# Scanner configuration
# Available scanners: PROMPT_GUARD, PII_DETECTION, HIDDEN_ASCII, AGENT_ALIGNMENT, CODE_SHIELD, MODERATION
# Example configurations:
# - Basic: {"USER": ["PROMPT_GUARD"]}
# - With PII detection: {"USER": ["PROMPT_GUARD", "PII_DETECTION"]}
# - With OpenAI moderation: {"USER": ["PROMPT_GUARD", "MODERATION"]}
# - Full protection: {"USER": ["PROMPT_GUARD", "MODERATION", "PII_DETECTION"]}
LLAMAFIREWALL_SCANNERS={"USER": ["PROMPT_GUARD"]}

# Tokenizer configuration (Optional)
# Set to false to disable tokenizer parallelism
# This can help with memory usage in some environments
TOKENIZERS_PARALLELISM=false

# Python configuration (Optional)
# These are set by default in the Dockerfile
# PYTHONDONTWRITEBYTECODE=1
# PYTHONUNBUFFERED=1
# PYTHONPATH=/app