services:
  api:
    build: .
    image: llamafirewall-api
    container_name: llamafirewall-api
    restart: unless-stopped
    ports:
      - "${PORT:-8000}:8000"
    env_file:
      - .env
    volumes:
      - huggingface_cache:/home/appuser/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:${PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - llamafirewall-net

networks:
  llamafirewall-net:
    driver: bridge

volumes:
  huggingface_cache:
    driver: local
